---
title: "DA2 Term Project - Survey Analysis - Cosmin Ticu"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Clear memory
rm(list=ls())
# Packages to use
library(tidyverse)
# For scaling ggplots
require(scales)
# Compare models with robust SE
#install.packages("texreg")
library(texreg)
# For different themes
#install.packages(ggthemes)
library(ggthemes)
library(mfx)
library(margins)
library(pscl)
library(modelsummary)
library(stargazer)
# install.packages("caTools")
library(caTools)
```

# Executive Summary

The aim of this study is to uncover the patterns of association between how office employees perceive their engagement in corporate safety missions and how they perceive various office safety factors using a probability model. With survey data stemming from a Fortune 100 company and a large (5000+) pool of respondents, training and test samples are used to validate findings. While the data and subsequent models are not fit for a classification task due to a heavily skewed response rate, due to a hypothesized few influential respondents, the study was able to draw minor predictive findings. Most notable of findings was that office employees who have not reviewed their workplace's hazards, who report not feeling safe in the office and who report not being familiar with office risks are less likely, on average, to feel engaged to a corporate safety mission. The findings of this study can be made more robust by minimizing the non-classical measurement error attributed to reactive "I agree" answers and by minizing the usage of observational variables, rather moving towards latent questions.

```{r, include=FALSE}
# Call the data
# w_dir <- "C:/Users/cosmi/Documents/homework_codesANDmore_Coding1_MScBA/Task3_Term-Project-Analysis/"
office_OG <- read.csv('https://raw.githubusercontent.com/cosmin-ticu/homework_codesANDmore_Coding1_MScBA/master/Task3_Term-Project-Analysis/data/clean/office_survey_data.csv')

# Split office data into train and test samples for robustness check at the end
set.seed(123)
sample <- sample.split(office_OG, SplitRatio = 0.75)
office <- subset(office_OG, sample == TRUE)
office_test <- subset(office_OG, sample == FALSE)

# Call Agoston's summary statistic function from a file:
source('https://raw.githubusercontent.com/cosmin-ticu/homework_codesANDmore_Coding1_MScBA/master/Task3_Term-Project-Analysis/codes/sum_stat.R')
```

# Introduction - Purpose & Rationale

The purpose of this study is to inspect the pattern of association between employees' perception of safety factors within their workspaces and how engaged they feel to their respective corporate safety missions, which ideally serve the purpose of protecting them and providing regulations for safe behavior. This safety is understood as a factor of incidents. Understood broadly, it refers to factors such as knowledge of fire escapes, injuries in the workplace, employee group behavior, knowledge of what to do during a crisis situation, etc. With an identified pattern of association, the ultimate goal of the study is to build a predictive statistical model which can allow us to estimate the trust or engagement of an employee into the safety measures preached by their employer. This can be extremely useful for companies that want to tackle making their workplaces feel safer to their employees. The research question can be summed up as: What are the factors that impact employees' engagement in corporate safety missions and how can we predict whether an employee will be engaged or not?

## Data collection

With the broad research idea in mind, the data for this study comes from a Fortune 100 global company in the form of a safety survey conducted on over 5000 employees in different working environments (from offices to manufacturing and services). The original survey contains answers to questions pertaining to employee safety within all the working environments (the original data with questions can be found [here](https://github.com/cosmin-ticu/homework_codesANDmore_Coding1_MScBA/blob/master/Task3_Term-Project-Analysis/data/original_survey_data.xlsx)). For the sake of narrowing down the scope of this analysis and based on the client's needs, the data was filtered (R script for cleaning can be found [here](https://github.com/cosmin-ticu/homework_codesANDmore_Coding1_MScBA/blob/master/Task3_Term-Project-Analysis/codes/clean_data.R)) to only contain the office environment questions. As such, the research question is narrowed down to office workplace safety. Nonetheless, for future analysis and robustness checks, the other environments can be employed. It is however, beyond the scope of this study.

The sample can be hypothesized to be representative in the case of the office environment, as we are dealing with over 1800 observations, each representing a single employee. The survey was anonymously distributed throughout the company, with any employee within the office environment having access to the survey. As such, the sampling distribution does not pose a hiccup in proceeding with generalizing findings. However, measurement errors do.

As with any survey data, classical measurement error is a concern for analysis. The survey was distributed over a longer period of time, without controlling for the environment that the employees were actually in while answering, while also allowing for employees to leave answers as blank or accidentally skip certain questions and have them marked as answered. As with classical measurement error, it is 0 on average, but it is assumed to be present within all the variables (thus both explanatory and dependent). That is especially prominent because most of the questions are observation-based, attempting to extract opinionated and biased answers from employees which should theoretically represent the truth. Ideally, this means that we can expect a less steep slope due to classical error in the explanatory variable(s) and a much larger standard error (or spread) in the predicted values and coefficients due to classical error in the dependent variable.

Lastly, non-classical measurement error is also hypothesized to be present with the data, as employees are inclined to always agree with any mission statements provided by their corporate employers. While the survey was anonymous, it is possible that people gave reactive answers, agreeing with the general mission statements and office attitudes. This type of error is much harder to identify and control, and it is suspected to be present and affect the distribution of predicted values (by skewing towards "yes" irrespective of explanatory parameters).

## The data

The variables of choice within the office environment sample are safe_office, hazards_reviewed, office_familiar_risks and safety_mission engagement.

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width='50%'}
# Descriptive statistics (bar charts) of variables of interest
ggplot(office) +
  geom_bar(aes(x = office_safety_mission_engagement), fill = 'blue3')+ 
  theme_bw() +
  ggtitle('Figure 1: Distribution of safety mission engagement answers')+
  theme(plot.title = element_text(size = 16))+
  labs(x = "Respondent feels engaged in the corporate safety mission",y = "Count of answers")
ggplot(office) +
  geom_bar(aes(x = safe_office), fill = 'red3')+ 
  theme_bw() +
  ggtitle('Figure 2: Distribution of office safety answers')+
  theme(plot.title = element_text(size = 16))+
  labs(x = "Respondent answers whether their office is safe",y = "Count of answers")
ggplot(office) +
  geom_bar(aes(x = office_familiar_risks), fill = 'paleturquoise')+ 
  theme_bw() +
  ggtitle('Figure 3: Distribution of office risks familiarity answers')+
  theme(plot.title = element_text(size = 16))+
  labs(x = "Respondent is familiar with their office's risks",y = "Count of answers")
ggplot(office) +
  geom_bar(aes(x = office_hazards_reviewed), fill = 'goldenrod3')+ 
  theme_bw() +
  ggtitle('Figure 4: Distribution of office hazard review answers')+
  theme(plot.title = element_text(size = 16))+
  labs(x = "Respondent has reviewed the hazards in their office",y = "Count of answers")
```

As per the research question, the safety_mission_engagement is chosen as the dependent variable, as it is of most interest to the client of this study. This study only employs binary categorical variables in saturated probability models. All the variables are transformed into binary and none of the observations are dropped, even if there are "no response" occurrences within the data. That is because the outcome variable only contains "yes" and "no" answers, thus making even a lack of response in one of the explanatory variables as potentially useful for uncovering the full pattern of association. Furthermore, with an almost 95% "yes" response to the safety mission engagement, it is paramount to find out just how influential those 5% of employees are who do not feel engaged. This will be uncovered by statistically significant coefficients. 

The Likert scale variable of office risk familiarity is transformed into multiple binary variables (baseline kept at "Familiar"). This is done instead of giving each entry a numeric value in order to reduce potential bias (because the difference between "Very familiar" and "Familiar" might not be quantitatively the same as between "Somewhat familiar" and "Not familiar"). Further summary statistics on each of the created binary variables can be found within Appendix A.1. 

What we can generally see, however, is that only the hazards_reviewed variable has a most even distribution between "yes" and "no" answers. This variable is hypothesized to suffer the least from classical measurement error and not be impacted by the non-classical error, because it is a latent variable (see its associated question in [original survey](https://github.com/cosmin-ticu/homework_codesANDmore_Coding1_MScBA/blob/master/Task3_Term-Project-Analysis/data/original_survey_data.xlsx)). 

With the variables of choice and the probability model framework in mind, we can proceed with creating the linear probability models. For this, training (75% or 1300 observations) and test (25% or 500 observations) samples have been created in order to benchmark the performance of the final model of choice against fresh data as a robustness check.

# Analysis and modelling

This section provides a walkthrough of the linear probability models (LPM) (a coefficient comparison of which is available [here](https://github.com/cosmin-ticu/homework_codesANDmore_Coding1_MScBA/blob/master/Task3_Term-Project-Analysis/output(s)/training_lpm_with_dummies.html)) and the logit and probit models used for actual prediction. First, an LPM rich model is built on which inferences are drawn. Then, it is used with bounded non-linear probability models to assess prediction and coverage (Pseudo R-Squared model summaries available [here](https://github.com/cosmin-ticu/homework_codesANDmore_Coding1_MScBA/blob/master/Task3_Term-Project-Analysis/output(s)/training_prob_models_coeff_R2_no_marginal.html) and model summaries with marginal differences for interpretation available [here](https://github.com/cosmin-ticu/homework_codesANDmore_Coding1_MScBA/blob/master/Task3_Term-Project-Analysis/output(s)/training_prob_models_coeff_with_marginal.html)).

## LPM

The simple LPM is built by regressing safety_mission_engagement on hazards_reviewed. As can be seen in the [model summary HTML](https://github.com/cosmin-ticu/homework_codesANDmore_Coding1_MScBA/blob/master/Task3_Term-Project-Analysis/output(s)/training_lpm_with_dummies.html), the simple LPM has a statistically significant slope coefficient. However, what the simple linear probability model does is that because 61 respondents who have not reviewed the office hazards also do not feel engaged in the safety mission, it just classifies all employees that have not reviewed the office hazards as also not being engaged in the safety mission. This can be clearly seen in Appendix A.2, comparing between A.2.1 and A.2.2. While this is a naive model, it sets a statistically significant basis on which to create a richer model.

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width='70%', fig.align='center'}
# 1st LPM model: safety mission engagement regressed on whether hazards have been reviewed
lpm1 <- lm( mission ~ hazards, data=office )

# Get the predicted values
office$pred1 <- predict( lpm1 )

# Use weights for prettier plot of first regression (important to see that distribution is heavily centered around "Yes" answers)
office<-office %>%
  group_by(mission, hazards) %>%
  mutate(weight = n())  %>%
  mutate(weight_2=(weight/1000))

ggplot(data = office) +
  geom_point( aes(x = hazards, y = pred1), size = 2, color="red3", shape = 16) +
  geom_line(  aes(x = hazards, y = pred1), colour="red3",  size = 0.7) +
  geom_point( aes(x = hazards, y = mission, size=weight_2), fill = "blue3", color="blue3",
              shape = 16, alpha=0.8, show.legend=F, na.rm=TRUE)  +
  labs(x = "Respondent has reviewed the hazards in the office", y = "Engaged to corporate safety mission / Predicted probability of ")+
  coord_cartesian(xlim = c(0, 1), ylim=c(0,1)) +
  scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.1))+
  scale_x_continuous(limits = c(0,1), breaks = seq(0,1,1))+ 
  theme_bw() +
  ggtitle('Figure 5: Simple LPM of mission on hazards')
```

The next 3 LPM models (found in the [analysis R script](https://github.com/cosmin-ticu/homework_codesANDmore_Coding1_MScBA/blob/master/Task3_Term-Project-Analysis/codes/analyze_data.R)) each add a new explantory variable used with the as.factor() function R to create dummy variables. The richest model, the 4th one, takes the form of regressing safety_mission_engagement on safe_office, hazards_reviewed and office_familiar_risks, thus containing all the variables of choice. This one also has the largest R-squared, which, even though not interpretable, means that it becomes the model of choice.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# 4th model: same as model 3 but with the office safety variable added
lpm4 <- lm( mission ~ hazards + as.factor(office_familiar_risks) + as.factor(safe_office), data=office )
```

## Model of choice - rich LPM

Because we have an upper boundary that is larger than 1 for these values, we need to use a probit or a logit model for prediction. However, for the purpose of uncovering patterns of association, we can proceed with using this rich LPM. Figure 6 below showcases the predicted probability distribution of our rich LPM model which we can use to inspect the bottom 10% of respondents and the top 10% of respondents to identify some key characteristics and differences.

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width='70%', fig.align='center'}
# Check predicted probabilities: is there any interesting values?
# predicted probabilities
office$pred_lpm_4 <- predict(lpm4)

# Show the predicted probabilities' distribution
ggplot(data=office, aes(x=pred_lpm_4)) +
  geom_histogram( aes( y = ..density.. ), fill = 'navyblue', binwidth=0.02) +
  coord_cartesian(xlim = c(0, 1.2)) +
  labs(x = "Predicted probability of respondent being engaged in the safety mission",y = "Percent")+ 
  theme_bw() +
  ggtitle('Figure 6: Predicted probability distribution of LPM rich')
```

The following interpretation is based on the summary statistics tables in Appendices A.3.2 to A.3.7. The bottom 10% means lower probability (starting from 60%) of safety mission engagement, while the top 10% of predicted values refers to a very high (also above 100% in the case of this unsaturated model) probability of safety mission engagement. A few interesting findings are that most of the people that reported their office as not safe are in the bottom 10% of predicted safety mission engagement, while in the top 10% we have the majority of respondents that did not answer the office safety question. There are only employees that are familiar with office risks in the top 10% of predicted safety mission engagement. It also appears that the hazard variable has a similar distribution for both the top 10% and the bottom 10% of predicted safety mission engagement, even though its coefficient is statistically significant in all the LPM models. These are useful findings because they pave the way for building probit and logit models in hopes of findings statistically significant coefficients (like the LPM models) but which are also viable for predictive analytics.

## Logit and probit models

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Lets compare
# lpm versus logit and probit
# with all right-hand-side variables
# If comparing different estimation methods for the same model setup:
#   good practice to make a 'formula' variable!
model_formula <- formula( mission ~ hazards + as.factor(office_familiar_risks) + as.factor(safe_office), data=office )

## 
# Logit coefficients
logit <- glm( model_formula , data=office, family=binomial(link="logit") )
# predicted probabilities 
office$pred_logit <- predict.glm(logit, type="response")
# Calculate logit marginal differences
logit_marg <- logitmfx( model_formula, data=office, atmean=FALSE, robust = T)

##
# Probit coefficients
probit <- glm(model_formula, data=office, family=binomial(link="probit"))
# predicted probabilities 
office$pred_probit<- predict.glm(probit, type="response") 
# probit marginal differences
probit_marg <- probitmfx(model_formula, data=office, atmean=FALSE)
```

The logit and probit models were run on the same formula used by the rich LPM and their predicted probabilities are shown with summary statistics in Appendix A.4.1. We can clearly see there that the non-linear models adjusted the upper boundary to 100%, while reducing the lower boundary to around 45% percent (43% logit and 47% probit). This gives us a larger spread of predicted values, which might attenuate some of the effects caused by the biased reactive "yes" answers in the survey or might showcase that the non-linear models tend to underestimate smaller probabilities and overestimate larger probabilities. This needs to be visualized in a comparison of predicted probabilities on a plot against the linear model.

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width='70%', fig.align='center'}
# Comparing predicted probabilities of logit and probit to LPM
ggplot(data = office) +
  geom_point(aes(x=pred_lpm_4, y=pred_probit, color="Probit"), size=1,  shape=16) +
  geom_point(aes(x=pred_lpm_4, y=pred_logit,  color="Logit"), size=1,  shape=16) +
  geom_line(aes(x=pred_lpm_4, y=pred_lpm_4,    color="45 degree line"), size=1) +
  labs(x = "Predicted probability of respondent safety mission engagement (LPM)", y="Predicted probability")+
  scale_y_continuous(expand = c(0.00,0.0), limits = c(0.3,1), breaks = seq(0,1,0.1)) +
  scale_x_continuous(expand = c(0.00,0.0), limits = c(0.3,1), breaks = seq(0,1,0.1)) +
  scale_color_manual(name = "", values=c("green", "red3","blue"))+
  theme(legend.position=c(0.55,0.08),
        legend.direction = "horizontal",
        legend.text = element_text(size = 7))+ 
  theme_bw() +
  ggtitle('Figure 7: Predicted probability plot between 3 competing models')
```

What we can see from Figure 7 is that the non-linear models appear to be underestimating the lower probabilities in comparison to the LPM rich model, while overestimating the higher probabilities. The only way to check if the models are suitable for prediction is to run a bias analysis and a calibration curve.

From the [model comparison with computed Pseudo R-squared](https://github.com/cosmin-ticu/homework_codesANDmore_Coding1_MScBA/blob/master/Task3_Term-Project-Analysis/output(s)/training_prob_models_coeff_R2_no_marginal.html) we can see that models 2 (logit) and 3 (probit) are very similar in terms of Pseudo R-squared, AIC and BIC, but very different in terms of coefficients. However, these coefficients cannot be interpreted by themselves, but rather they need to be converted into marginal differences. The comparison between models with marginal differences can be found [here](https://github.com/cosmin-ticu/homework_codesANDmore_Coding1_MScBA/blob/master/Task3_Term-Project-Analysis/output(s)/training_prob_models_coeff_with_marginal.html).

From this point on, because of the similarity between the two non-linear models, the logit is arbitrarily chosen as the model of choice. Before interpreting its coefficients, we need to inspect its goodness of fit with regards to the earliest naive model, in order to see if the dispersion (distribution) of fitted values shows a clearer distinction between the respondents that actually said they feel engaged in the safety mission and the ones that do not feel that way.

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width='50%'}
# Simple LPM model from first regression
ggplot(data = office,aes(x=pred1)) + 
  geom_histogram(data=subset(office[office$mission == 1, ]), 
                 aes(fill=as.factor(mission), color=as.factor(mission), y = (..count..)/sum(..count..)*100),
                 binwidth = 0.05, boundary=0, alpha=0.8) +
  geom_histogram(data=subset(office[office$mission == 0, ]), 
                 aes(fill=as.factor(mission), color=as.factor(mission), y = (..count..)/sum(..count..)*100), 
                 binwidth = 0.05, boundary=0, alpha=0) +
  scale_fill_manual(name="", values=c("0" = "white", "1" = "red3"),labels=c("Not engaged in safety mission","Respondent engaged in safety mission")) +
  scale_color_manual(name="", values=c("0" = "blue3", "1" = "red3"),labels=c("Not engaged in safety mission","Respondent engaged in safety mission")) +
  ylab("Percent") +
  xlab("Fitted values") +
  scale_x_continuous(expand=c(0.01,0.01) ,limits = c(0,1), breaks = seq(0,1,0.2)) +
  scale_y_continuous(expand=c(0.00,0.00) ,limits = c(0,100), breaks = seq(0,100,20)) +
  theme(legend.position = c(0.3,0.9),
        legend.key.size = unit(x = 0.5, units = "cm"))+ 
  theme_bw() +
  ggtitle('Figure 8: Distribution of simple LPM predicted probabilities by outcome')

# Logit model based on LPM rich formula
ggplot(data = office,aes(x=pred_logit)) + 
  geom_histogram(data=subset(office[office$mission == 1, ]), 
                 aes(fill=as.factor(mission), color=as.factor(mission), y = (..count..)/sum(..count..)*100),
                 binwidth = 0.05, boundary=0, alpha=0.8) +
  geom_histogram(data=subset(office[office$mission == 0, ]), 
                 aes(fill=as.factor(mission), color=as.factor(mission), y = (..count..)/sum(..count..)*100), 
                 binwidth = 0.05, boundary=0, alpha=0) +
  scale_fill_manual(name="", values=c("0" = "white", "1" = "red3"),labels=c("Not engaged in safety mission","Respondent engaged in safety mission")) +
  scale_color_manual(name="", values=c("0" = "blue3", "1" = "red3"),labels=c("Not engaged in safety mission","Respondent engaged in safety mission")) +
  ylab("Percent") +
  xlab("Fitted values") +
  scale_x_continuous(expand=c(0.01,0.01) ,limits = c(0,1), breaks = seq(0,1,0.2)) +
  scale_y_continuous(expand=c(0.00,0.00) ,limits = c(0,100), breaks = seq(0,100,20)) +
  theme(legend.justification = c("right", "top"),
        legend.key.size = unit(x = 0.5, units = "cm"))+ 
  theme_bw() +
  ggtitle('Figure 9: Distribution of logit predicted probabilities by outcome')
```

Figures 8 and 9 show a clear difference between the naive linear probability model and the logit model. We can see that the distribution is wider between actual "no" outcomes and "yes" outcomes within the logit model's predicted values. However, there is still noticeable overlap between the two. This tendency towards the "yes" outcome, perhaps even due to the non-classical measurement error, makes this model unfit for a classification task, as the logical 50% boundary would result in a mere few instances being categorized accurately for the "no" outcome, much less than the actual number. From the looks of this model, it would have a tendency to commit few false negatives at the expense of committing many false positives. It makes the statistically significant coefficients all the more interesting to inspect. For future analysis purposes, it would be interesting to investigate whether there a few influential observations, which might actually in turn enable the statistically significant but fairly small slope coefficients. Lastly, as can be observed in the summary statistics on conditional predicted probabilities in appendices A.4.2 and A.4.3, the predicted probability means between the two outcome groups ("yes" table versus "no" table) are extremely close to each other.

## Bias & calibration curve

With a bias around 0.0000000006, we can safely dismiss concerns about a biased predicted sample. Furthermore, as can be seen in appendix A.5.1, the calibration curve (even with 15 bins) shows probabilities scattered around their actual ones, with a few influential or more scattered points showing a few abrupt patterns. Nonetheless, the logit model's calibration curve is close enough to the 45-degree line in order to warrant this model a fairly calibrated mark. This means that we can carefully proceed with the final model interpretation.

# Final model & Interpretation

Logit regression on LPM rich formula: **mission = link_function(1.452 + 0.937 * hazards + 14.384 * no_response_familiar_risks - 1.722 * not_familiar_risks - 0.891 * somewhat_familiar_risks + 0.429 * very_familiar_risks + 16.041 * no_respose_safe_office + 1.548 * yes_safe_office)**

Based on the marginal differences computed [here](https://github.com/cosmin-ticu/homework_codesANDmore_Coding1_MScBA/blob/master/Task3_Term-Project-Analysis/output(s)/training_prob_models_coeff_with_marginal.html), we can proceed to draw interesting findings from the interpretation of only the statistically significant coefficients. 

As such, office employees in the context of this study that answer that they believe their office is safe are 13.3% more likely, on average, to answer that they feel engaged in the corporate safety mission, if we control for their answers on whether the office hazards have been reviewed and for their answers on whether they are familiar with the office risks. Another finding would be that office employees in the context of this study that say they are not familiar with their office's risks, that say they are not safe in the office and that say they have not reviewed their office's hazards are 15% less likely to answer that they feel engaged in the corporate safety mission. These findings show that, while the model is not suitable for a classification task due to the heavy bias towards the "yes" answers, the few possibly influential observations (as respondents who do not feel engaged to the corporate safety mission) have uncovered some interesting patterns of association between perceived office environment safety and perceived engagement to a corporate mission.

## Robustness check

As a final step of this study, a series of robustness checks was conducted on the test sample of the data that was created as a 25% representative sample of the original office data. The summary statistics (appendix A.6.1) of the test sample predicted values appear very similar to their counterparts in the training sample (appendix A.4.1). The calibration curve of the logit model run on the test data (appendix A.6.2) shows less calibration, while the comparison between distribution of logit predicted probabilities by outcome on each of the samples shows a distribution on the test sample much more similar to the original simple LPM naive model, rather than the richer logit model. As a last robustness check, it appears that, while not all of the coefficients have retained their significance level, most of the test sample coefficients are within 2 standard errors away from their counterparts in the training sample. On a concluding remark, perhaps taking a 25% test sample of a sample already 95%-dominated by "yes" mission statement engagement answers has made the few influential points too far and few.

\newpage

# Appendix A

This appendix serves the purpose of storing and displaying tables, charts, graphs and model comparisons, all of which supplement the main body of this text. References to the appendix can be found throughout the study.

## A.1: Descriptive Statistics

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Make descriptive statistics for selected office variables
desc_stat_office <- sum_stat( office , var_names = c('yes_safe_office','no_safe_office','noresponse_safe_office','hazards','mission'),
                       stats = c('mean','median','min','max','sd') ) # likert scale variables are ignored for this
knitr::kable(desc_stat_office, caption = "A.1.1: Descriptive statistics of the chosen variables in the office-side survey")

desc_office_risks <- table(office$office_familiar_risks)
knitr::kable(desc_office_risks, caption = "A.1.2: Frequency distribution of the office risks familiarity variable (Likert scale)")
```

## A.2: Simple LPM Model

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Compare hazard variable with predicted values and real outcomes
knitr::kable(table(office$mission, office$hazards), caption = "A.2.1: Confusion table showing distribution of actual outcome (rows) and explanatory (columns)")
knitr::kable(table(office$pred1, office$hazards), caption = "A.2.2: Confusion table showing distribution of predicted outcome (rows) and explanatory (columns)")

```

## A.3: Model of choice - rich LPM

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Summary statistics of predicted values
knitr::kable(sum_stat(office, 'pred_lpm_4', c('mean','median','sd','min','max')), caption = "A.3.1: Summary statistics LPM rich predicted values")

# We are interested in the top 10% and bottom 10% characteristics!
#   Is there any significant difference?

# Create bins which categorize the predicted values between 1-10
office <- office %>% 
  mutate(q10_pred_lpm_4 = ntile(pred_lpm_4, 10))

# Make summary statistics, using sum_stat for the bottom (q10_pred_lpm==1) 
#   and top 10% (q10_pred_lpm==10), using stats = c('mean','median','sd')

# Bottom 10% means lower probability (starting from 60%) of safety mission engagement
b1 <- office %>% filter( q10_pred_lpm_4 == 1 )
var_interest <- 'hazards'
stat_interest <- c('mean','median','sd')

knitr::kable(sum_stat(b1,var_interest,stat_interest,num_obs = F), caption = "A.3.2: Summary statistics of hazards variable on the bottom 10% of respondents")

knitr::kable(table(b1$office_familiar_risks), caption = "A.3.3: Summary statistics of office_familiar_risks variable on the bottom 10% of respondents")

knitr::kable(table(b1$safe_office), caption = "A.3.4: Summary statistics of safe_office variable on the bottom 10% of respondents") # most of the people that reported their office as not safe are in the bottom 10% of safety mission engagement

# Top 10% means high probability of safety mission engagement
t1 <- office %>% filter( q10_pred_lpm_4 == 10 )

knitr::kable(sum_stat(t1,var_interest,stat_interest,num_obs = F), caption = "A.3.5: Summary statistics of hazards variable on the top 10% of respondents")

knitr::kable(table(t1$office_familiar_risks), caption = "A.3.6: Summary statistics of office_familiar_risks variable on the top 10% of respondents") # none of the employees that are not familiar with office risks are in the top 10% of safety mission engagement

knitr::kable(table(t1$safe_office), caption = "A.3.7: Summary statistics of safe_office variable on the top 10% of respondents")
```

## A.4 Probit & Logit models

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Summary statistics of predicted values for logit
sum_stat_logit <- sum_stat(office, 'pred_logit', c('mean','median','sd','min','max'))
# Summary statistics of predicted values for logit
sum_stat_probit <- sum_stat(office, 'pred_probit', 
                            c('mean','median','sd','min','max')) %>% subset(select = -statistics)
# Summary statistics of predcited values for rich LPM
sum_stat_lpm <- sum_stat(office, 'pred_lpm_4', 
                         c('mean','median','sd','min','max')) %>% subset(select = -statistics)

# combine the summary tables of x and y variables under one streamlined table
table_summary <- add_column(sum_stat_logit,sum_stat_probit, sum_stat_lpm)
knitr::kable(table_summary,caption="A.4.1: Summary statistics for the predicted values of the LPM, logit and probit models of choice")

#   Create a CONDITIONAL sum_stat on mission for:
#     "pred_lpm","pred_logit","pred_probit" 
#   using: "mean","median","min","max","sd"
#
ss_1 <- subset( office , office$mission==1 )
ss_0 <- subset( office , office$mission==0 )

ss_1s <- sum_stat(ss_1,c("pred_lpm_4","pred_logit","pred_probit"),
                  c("mean","median","min","max","sd"),num_obs = F)
ss_0s <- sum_stat(ss_0,c("pred_lpm_4","pred_logit","pred_probit"),
                  c("mean","median","min","max","sd"),num_obs = F)
knitr::kable(ss_1s, caption = "A.4.2: Outcome conditional ('yes') summary statistics for the predicted values of models of choice")
knitr::kable(ss_0s, caption = "A.4.3: Outcome conditional ('no') summary statistics for the predicted values of models of choice")
```

## A.5 Bias & calibration curve

```{r, echo=FALSE, warning=FALSE, message=FALSE}
###
# Bias and Calibration curve
#
# use the logit model
#
# bias = mean(prediction) - mean(actual)
mean_pred_logit <- mean( office$pred_logit )
bias <- mean_pred_logit - mean( office$mission )
# Not really biased... it is really tiny!


# Note dplyr:: is important to specify which package's 'select' is used!
actual_vs_predicted <- office %>%
  ungroup() %>% 
  dplyr::select(actual = mission, 
                predicted = pred_logit) 
num_groups <- 15

calibration_d <- actual_vs_predicted %>%
  mutate(predicted_score_group = dplyr::ntile(predicted, num_groups))%>%
  group_by(predicted_score_group) %>%
  dplyr::summarise(mean_actual = mean(actual), 
                   mean_predicted = mean(predicted), 
                   num_obs = n())

ggplot( calibration_d,aes(x = mean_actual, y = mean_predicted)) +
  geom_point( color='red3', size=1.5, alpha=0.8) +
  geom_line(  color='red3', size=1  , alpha=0.8) +
  geom_abline( intercept = 0, slope = 1, color='blue3') +
  labs( x = "Actual event probability", y = "Predicted event probability") +
  scale_x_continuous(expand = c(0.01,0.01), limits = c(0.5,1), breaks = seq(0,1,0.1)) +
  scale_y_continuous(expand = c(0.01,0.01), limits = c(0.5,1), breaks = seq(0,1,0.1))+ 
  theme_bw() +
  ggtitle('A.5.1: Logit model calibration curve')

```

## A.6 Robustness check

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Run model of choice on test sample --------------------------------------

# Because the goal is to predict, the logit model is chosen to run the test sample
# and compare the findings to the training sample

logit_test <- glm( model_formula , data=office_test, family=binomial(link="logit") )

# predicted probabilities 
office_test$pred_logit <- predict.glm(logit_test, type="response")
# Summary statistics on predicted values with the test sample
knitr::kable(sum_stat(office_test, 'pred_logit', c('mean','median','sd','min','max')), caption = "A.6.1: Test sample - Summary statistics logit predicted values")

# Calculate logit marginal differences
logit_marg_test <- logitmfx( model_formula, data=office_test, atmean=FALSE, robust = T)

###
# Bias and Calibration curve for the test sample
#
# using the logit model
#
# bias = mean(prediction) - mean(actual)
mean_pred_logit_test <- mean( office_test$pred_logit )
bias_test <- mean_pred_logit_test - mean( office_test$mission )
# Not really biased... it is really tiny!

# Note dplyr:: is important to specify which package's 'select' is used!
actual_vs_predicted_test <- office_test %>%
  ungroup() %>% 
  dplyr::select(actual = mission, 
                predicted = pred_logit) 
num_groups <- 10

calibration_d_test <- actual_vs_predicted_test %>%
  mutate(predicted_score_group = dplyr::ntile(predicted, num_groups))%>%
  group_by(predicted_score_group) %>%
  dplyr::summarise(mean_actual = mean(actual), 
                   mean_predicted = mean(predicted), 
                   num_obs = n())

ggplot( calibration_d_test,aes(x = mean_actual, y = mean_predicted)) +
  geom_point( color='red3', size=1.5, alpha=0.8) +
  geom_line(  color='red3', size=1  , alpha=0.8) +
  geom_abline( intercept = 0, slope = 1, color='blue3') +
  labs( x = "Actual event probability", y = "Predicted event probability") +
  scale_x_continuous(expand = c(0.01,0.01), limits = c(0.5,1), breaks = seq(0,1,0.1)) +
  scale_y_continuous(expand = c(0.01,0.01), limits = c(0.5,1), breaks = seq(0,1,0.1))+ 
  theme_bw() +
  ggtitle('A.6.2: Test sample - Logit model calibration curve')
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width='50%'}
# Training sample - Logit model based on LPM rich formula
ggplot(data = office,aes(x=pred_logit)) + 
  geom_histogram(data=subset(office[office$mission == 1, ]), 
                 aes(fill=as.factor(mission), color=as.factor(mission), y = (..count..)/sum(..count..)*100),
                 binwidth = 0.05, boundary=0, alpha=0.8) +
  geom_histogram(data=subset(office[office$mission == 0, ]), 
                 aes(fill=as.factor(mission), color=as.factor(mission), y = (..count..)/sum(..count..)*100), 
                 binwidth = 0.05, boundary=0, alpha=0) +
  scale_fill_manual(name="", values=c("0" = "white", "1" = "red3"),labels=c("Not engaged in safety mission","Respondent engaged in safety mission")) +
  scale_color_manual(name="", values=c("0" = "blue3", "1" = "red3"),labels=c("Not engaged in safety mission","Respondent engaged in safety mission")) +
  ylab("Percent") +
  xlab("Fitted values") +
  scale_x_continuous(expand=c(0.01,0.01) ,limits = c(0,1), breaks = seq(0,1,0.2)) +
  scale_y_continuous(expand=c(0.00,0.00) ,limits = c(0,100), breaks = seq(0,100,20)) +
  theme(legend.justification = c("right", "top"),
        legend.key.size = unit(x = 0.5, units = "cm"))+ 
  theme_bw() +
  ggtitle('A.6.3: Training sample - logit predicted probabilities by outcome')

####
# checking the goodness of fit of the logit model on the test data
#
ggplot(data = office_test,aes(x=pred_logit)) + 
  geom_histogram(data=subset(office_test[office_test$mission == 1, ]), 
                 aes(fill=as.factor(mission), color=as.factor(mission), y = (..count..)/sum(..count..)*100),
                 binwidth = 0.05, boundary=0, alpha=0.8) +
  geom_histogram(data=subset(office_test[office_test$mission == 0, ]), 
                 aes(fill=as.factor(mission), color=as.factor(mission), y = (..count..)/sum(..count..)*100), 
                 binwidth = 0.05, boundary=0, alpha=0) +
  scale_fill_manual(name="", values=c("0" = "white", "1" = "red3"),labels=c("Not engaged in safety mission","Respondent engaged in safety mission")) +
  scale_color_manual(name="", values=c("0" = "blue3", "1" = "red3"),labels=c("Not engaged in safety mission","Respondent engaged in safety mission")) +
  ylab("Percent") +
  xlab("Fitted values") +
  scale_x_continuous(expand=c(0.01,0.01) ,limits = c(0,1), breaks = seq(0,1,0.2)) +
  scale_y_continuous(expand=c(0.00,0.00) ,limits = c(0,100), breaks = seq(0,100,20)) +
  theme(legend.position = c(0.3,0.9),
        legend.key.size = unit(x = 0.5, units = "cm"))+ 
  theme_bw() +
  ggtitle('A.6.4: Test sample - logit predicted probabilities by outcome')
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#####
# Summary statistics on predicted probabilities from the test sample:
#
ss_1_test <- subset( office_test , office_test$mission==1 )
ss_0_test <- subset( office_test , office_test$mission==0 )

ss_1s_test <- sum_stat(ss_1_test,"pred_logit",
                  c("mean","median","min","max","sd"),num_obs = F)
ss_0s_test <- sum_stat(ss_0_test,"pred_logit",
                  c("mean","median","min","max","sd"),num_obs = F)
knitr::kable(ss_1s_test, caption = "A.6.5: Test sample - Outcome conditional ('yes') summary statistics for the predicted values of models of choice")
knitr::kable(ss_0s_test, caption = "A.6.6: Test sample - Outcome conditional ('no') summary statistics for the predicted values of models of choice")
```
